{"root":{"data":{"id":"d9d52iua5dc0","created":1745322158637,"text":"集成学习算法"},"children":[{"data":{"id":"d9d52muu3c80","created":1745322167378,"text":"Bagging（Bootstrap Aggregating）"},"children":[{"data":{"id":"d9d5384eu0g0","created":1745322213669,"text":"原理"},"children":[{"data":{"id":"d9d53ednclc0","created":1745322227288,"text":"Bagging方法通过对数据集进行有放回抽样生成多个不同的训练集，然后在每个训练集上训练一个独立的基础模型，并将这些模型的结果综合起来作为最终输出。"},"children":[]}]},{"data":{"id":"d9d53iga0r40","created":1745322236155,"text":"代表算法"},"children":[{"data":{"id":"d9d53klqu880","created":1745322240839,"text":"随机森林"},"children":[{"data":{"id":"d9d53obvv7s0","created":1745322248950,"text":"一种基于决策树的bagging方法，除了对样本进行抽样外，还会在构建每棵树时对特征进行随机选择，以增加模型间的多样性。"},"children":[]}]}]},{"data":{"id":"d9d5404qm7k0","created":1745322274639,"text":"优点"},"children":[{"data":{"id":"d9d542ac3p40","created":1745322279331,"text":"减少方差，降低过拟合的风险。"},"children":[]}]},{"data":{"id":"d9d545cwlyw0","created":1745322286017,"text":"缺点"},"children":[{"data":{"id":"d9d54875p8g0","created":1745322292200,"text":"对于偏差高的基础模型效果有限。"},"children":[]}]}]},{"data":{"id":"d9d54awtcrk0","created":1745322298104,"text":"Boosting"},"children":[{"data":{"id":"d9d5384eu0g0","created":1745322213669,"text":"原理"},"children":[{"data":{"id":"d9d555uhzj40","created":1745322365445,"text":"Boosting是一种迭代方法，它按照顺序训练一系列基础模型，其中每一个新模型都会尝试纠正前面模型的错误。通常，前一个模型的误差会影响下一个模型的权重或关注点。"},"children":[]}]},{"data":{"id":"d9d53iga0r40","created":1745322236155,"text":"代表算法"},"children":[{"data":{"id":"d9d558c7pl40","created":1745322370869,"text":"AdaBoost（Adaptive Boosting）"},"children":[{"data":{"id":"d9d55acvlpc0","created":1745322375263,"text":"通过调整被错误分类实例的权重来逐步改善模型性能。"},"children":[]}]},{"data":{"id":"d9d55dtvi5s0","created":1745322382821,"text":"梯度提升（Gradient Boosting）"},"children":[{"data":{"id":"d9d55g0ojgw0","created":1745322387586,"text":"包括XGBoost、LightGBM和CatBoost等实现，通过梯度下降的方式最小化损失函数。"},"children":[]}]}]},{"data":{"id":"d9d5404qm7k0","created":1745322274639,"text":"优点"},"children":[{"data":{"id":"d9d55i0tyf40","created":1745322391949,"text":"有效减少偏差，适合处理复杂的非线性关系。"},"children":[]}]},{"data":{"id":"d9d545cwlyw0","created":1745322286017,"text":"缺点"},"children":[{"data":{"id":"d9d55jkjtrk0","created":1745322395318,"text":"可能对噪声敏感，容易过拟合。"},"children":[]}]}]},{"data":{"id":"d9d54pylx0g0","created":1745322330866,"text":"Stacking"},"children":[{"data":{"id":"d9d5384eu0g0","created":1745322213669,"text":"原理"},"children":[{"data":{"id":"d9d55m1o3n40","created":1745322400707,"text":"Stacking涉及训练多个不同类型的模型（称为第一层或基模型），然后使用另一个模型（称为元模型或第二层模型）来整合这些基模型的预测结果。"},"children":[]}]},{"data":{"id":"d9d53iga0r40","created":1745322236155,"text":"代表算法"},"children":[]},{"data":{"id":"d9d5404qm7k0","created":1745322274639,"text":"优点"},"children":[]},{"data":{"id":"d9d545cwlyw0","created":1745322286017,"text":"缺点"},"children":[]}]},{"data":{"id":"d9d54w4ochc0","created":1745322344292,"text":"Blending"},"children":[{"data":{"id":"d9d5384eu0g0","created":1745322213669,"text":"原理"},"children":[]},{"data":{"id":"d9d53iga0r40","created":1745322236155,"text":"代表算法"},"children":[]},{"data":{"id":"d9d5404qm7k0","created":1745322274639,"text":"优点"},"children":[]},{"data":{"id":"d9d545cwlyw0","created":1745322286017,"text":"缺点"},"children":[]}]},{"data":{"id":"d9d54ysyg0w0","created":1745322350114,"text":" Voting（投票法）"},"children":[{"data":{"id":"d9d5384eu0g0","created":1745322213669,"text":"原理"},"children":[]},{"data":{"id":"d9d53iga0r40","created":1745322236155,"text":"代表算法"},"children":[]},{"data":{"id":"d9d5404qm7k0","created":1745322274639,"text":"优点"},"children":[]},{"data":{"id":"d9d545cwlyw0","created":1745322286017,"text":"缺点"},"children":[]}]}]},"template":"right","theme":"fresh-blue","version":"1.4.43"}